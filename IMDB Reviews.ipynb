{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import re\n",
    "import sys\n",
    "from hashlib import sha1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Autograding\n",
    "import tests_lab4\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# train test split and cross validation\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Sentiment analysis on the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-13d9bdc07140534d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>train</td>\n",
       "      <td>Story of a man who has unnatural feelings for ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25001</th>\n",
       "      <td>train</td>\n",
       "      <td>Airport '77 starts as a brand new luxury 747 p...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10000_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25002</th>\n",
       "      <td>train</td>\n",
       "      <td>This film lacked something I couldn't put my f...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10001_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25003</th>\n",
       "      <td>train</td>\n",
       "      <td>Sorry everyone,,, I know this is supposed to b...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10002_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25004</th>\n",
       "      <td>train</td>\n",
       "      <td>When I was little my parents took me along to ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10003_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>train</td>\n",
       "      <td>Seeing as the vote average was pretty low, and...</td>\n",
       "      <td>pos</td>\n",
       "      <td>9998_9.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>train</td>\n",
       "      <td>The plot had some wretched, unbelievable twist...</td>\n",
       "      <td>pos</td>\n",
       "      <td>9999_8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>train</td>\n",
       "      <td>I am amazed at how this movie(and most others ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>999_10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>train</td>\n",
       "      <td>A Christmas Together actually came before my t...</td>\n",
       "      <td>pos</td>\n",
       "      <td>99_8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>train</td>\n",
       "      <td>Working-class romantic drama from director Mar...</td>\n",
       "      <td>pos</td>\n",
       "      <td>9_7.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        type                                             review label  \\\n",
       "25000  train  Story of a man who has unnatural feelings for ...   neg   \n",
       "25001  train  Airport '77 starts as a brand new luxury 747 p...   neg   \n",
       "25002  train  This film lacked something I couldn't put my f...   neg   \n",
       "25003  train  Sorry everyone,,, I know this is supposed to b...   neg   \n",
       "25004  train  When I was little my parents took me along to ...   neg   \n",
       "...      ...                                                ...   ...   \n",
       "49995  train  Seeing as the vote average was pretty low, and...   pos   \n",
       "49996  train  The plot had some wretched, unbelievable twist...   pos   \n",
       "49997  train  I am amazed at how this movie(and most others ...   pos   \n",
       "49998  train  A Christmas Together actually came before my t...   pos   \n",
       "49999  train  Working-class romantic drama from director Mar...   pos   \n",
       "\n",
       "              file  \n",
       "25000      0_3.txt  \n",
       "25001  10000_4.txt  \n",
       "25002  10001_4.txt  \n",
       "25003  10002_1.txt  \n",
       "25004  10003_1.txt  \n",
       "...            ...  \n",
       "49995   9998_9.txt  \n",
       "49996   9999_8.txt  \n",
       "49997   999_10.txt  \n",
       "49998     99_8.txt  \n",
       "49999      9_7.txt  \n",
       "\n",
       "[25000 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df = pd.read_csv(\"imdb_master.csv\", encoding=\"ISO-8859-1\", index_col=\"Unnamed: 0\")\n",
    "imdb_df = imdb_df.query('label == \"neg\" | label == \"pos\"')\n",
    "train_df = imdb_df.query('type == \"train\"')\n",
    "test_df = imdb_df.query('type == \"test\"')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-af9ebf01a716d388",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = train_df[\"review\"], train_df[\"label\"]\n",
    "X_test, y_test = test_df[\"review\"], test_df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas_profiling import ProfileReport\n",
    "# ProfileReport(train_df.query(\"label == 'pos'\"), explorative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "negative reviews length: \n",
    "Max length 8969, Median length\t976.5, Mean length 1303.19936, Min length 52\n",
    "\n",
    "positive reviews length: \n",
    "Max length 13704, Median length 982, Mean length 1347.42648, Min length\t70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building and hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted code\n",
    "def store_cross_val_results(model_name, scores, results_dict):\n",
    "    \"\"\"\n",
    "    Stores mean scores from cross_validate in results_dict for\n",
    "    the given model model_name.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name :\n",
    "        scikit-learn classification model\n",
    "    scores : dict\n",
    "        object return by `cross_validate`\n",
    "    results_dict: dict\n",
    "        dictionary to store results\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        None\n",
    "\n",
    "    \"\"\"\n",
    "    results_dict[model_name] = {\n",
    "        \"mean_train_accuracy\": \"{:0.4f}\".format(np.mean(scores[\"train_score\"])),\n",
    "        \"mean_valid_accuracy\": \"{:0.4f}\".format(np.mean(scores[\"test_score\"])),\n",
    "        \"mean_fit_time (s)\": \"{:0.4f}\".format(np.mean(scores[\"fit_time\"])),\n",
    "        \"mean_score_time (s)\": \"{:0.4f}\".format(np.mean(scores[\"score_time\"])),\n",
    "        \"std_train_score\": \"{:0.4f}\".format(scores[\"train_score\"].std()),\n",
    "        \"std_valid_score\": \"{:0.4f}\".format(scores[\"test_score\"].std()),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline DummyClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy=\"prior\")\n",
    "dummy_pipe = make_pipeline(CountVectorizer(binary=True), DummyClassifier(strategy=\"most_frequent\"))\n",
    "scores = cross_validate(dummy_pipe, X_train, y_train, return_train_score=True)\n",
    "store_cross_val_results(\"dummy\", scores, results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dummy': {'mean_train_accuracy': '0.5000',\n",
       "  'mean_valid_accuracy': '0.5000',\n",
       "  'mean_fit_time (s)': '4.2760',\n",
       "  'mean_score_time (s)': '1.0344',\n",
       "  'std_train_score': '0.0000',\n",
       "  'std_valid_score': '0.0000'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"RBF SVM\": SVC(),\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=2000),\n",
    "}\n",
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    pipe = Pipeline(steps=[(\"pca\", CountVectorizer(binary = True)), (\"classifier\", model)])\n",
    "    scores = cross_validate(pipe, X_train, y_train, cv=2, return_train_score=True, n_jobs=-1)\n",
    "    store_cross_val_results(model_name, scores, results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_accuracy</th>\n",
       "      <th>mean_valid_accuracy</th>\n",
       "      <th>mean_fit_time (s)</th>\n",
       "      <th>mean_score_time (s)</th>\n",
       "      <th>std_train_score</th>\n",
       "      <th>std_valid_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.7050</td>\n",
       "      <td>13.8674</td>\n",
       "      <td>2.7347</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RBF SVM</th>\n",
       "      <td>0.9738</td>\n",
       "      <td>0.8646</td>\n",
       "      <td>177.0820</td>\n",
       "      <td>115.5021</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.9377</td>\n",
       "      <td>0.8088</td>\n",
       "      <td>3.1778</td>\n",
       "      <td>2.6625</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.8520</td>\n",
       "      <td>5.3049</td>\n",
       "      <td>2.6935</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mean_train_accuracy mean_valid_accuracy mean_fit_time (s)  \\\n",
       "Decision Tree                    1.0000              0.7050           13.8674   \n",
       "RBF SVM                          0.9738              0.8646          177.0820   \n",
       "Naive Bayes                      0.9377              0.8088            3.1778   \n",
       "Logistic Regression              0.9994              0.8520            5.3049   \n",
       "\n",
       "                    mean_score_time (s) std_train_score std_valid_score  \n",
       "Decision Tree                    2.7347          0.0000          0.0014  \n",
       "RBF SVM                        115.5021          0.0002          0.0018  \n",
       "Naive Bayes                      2.6625          0.0058          0.0117  \n",
       "Logistic Regression              2.6935          0.0001          0.0004  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results_dict).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The LogReg and RBF SVM are very close, with high validation scores. Then the NB, and followed by DT. \n",
    "\n",
    "> The distance between the train and validation scores for RBF are approx 10, for NB is 13, for LogReg is 14, and for DT is 30. So the RBF is actually fitting the best.\n",
    "\n",
    "> In terms of timing, the NB and LogReg fit very fast, with RBF being around 30x to 50x longer, and DT around 5x longer. \n",
    "\n",
    "> The only one that seems to be severely overfitting is the DT, which is expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "pipe_3_3 = Pipeline(steps=[(\"pca\", CountVectorizer(binary = True)), \n",
    "                           (\"classifier\", LogisticRegression(max_iter = 1000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=2,\n",
       "                   estimator=Pipeline(steps=[('pca',\n",
       "                                              CountVectorizer(binary=True)),\n",
       "                                             ('classifier',\n",
       "                                              LogisticRegression(max_iter=1000))]),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'classifier__C': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02]),\n",
       "                                        'pca__max_features': [3500, 7500,\n",
       "                                                              1000]},\n",
       "                   return_train_score=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid_random = {\"classifier__C\": 10.0 ** np.arange(-3, 3),\n",
    "                 \"pca__max_features\": [3500,7500,1000]}\n",
    "\n",
    "random_searching = RandomizedSearchCV(pipe_3_3, \n",
    "                                      param_distributions=param_grid_random, \n",
    "                                      n_jobs=-1, n_iter=10, return_train_score=True, cv=2)\n",
    "random_searching.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_classifier__C</th>\n",
       "      <th>param_pca__max_features</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.346765</td>\n",
       "      <td>0.86228</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.902316</td>\n",
       "      <td>0.85836</td>\n",
       "      <td>0.01</td>\n",
       "      <td>7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.288775</td>\n",
       "      <td>0.84536</td>\n",
       "      <td>1</td>\n",
       "      <td>7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.772601</td>\n",
       "      <td>0.84408</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.905560</td>\n",
       "      <td>0.83888</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.741022</td>\n",
       "      <td>0.83840</td>\n",
       "      <td>100</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.166258</td>\n",
       "      <td>0.83088</td>\n",
       "      <td>0.001</td>\n",
       "      <td>7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.709755</td>\n",
       "      <td>0.83000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.676651</td>\n",
       "      <td>0.81868</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.915322</td>\n",
       "      <td>0.80696</td>\n",
       "      <td>100</td>\n",
       "      <td>3500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mean_fit_time  mean_test_score param_classifier__C  \\\n",
       "rank_test_score                                                       \n",
       "1                     5.346765          0.86228                 0.1   \n",
       "2                     4.902316          0.85836                0.01   \n",
       "3                     6.288775          0.84536                   1   \n",
       "4                     4.772601          0.84408                0.01   \n",
       "5                     7.905560          0.83888                  10   \n",
       "6                     6.741022          0.83840                 100   \n",
       "7                     6.166258          0.83088               0.001   \n",
       "8                     5.709755          0.83000               0.001   \n",
       "9                     5.676651          0.81868               0.001   \n",
       "10                   10.915322          0.80696                 100   \n",
       "\n",
       "                param_pca__max_features  \n",
       "rank_test_score                          \n",
       "1                                  7500  \n",
       "2                                  7500  \n",
       "3                                  7500  \n",
       "4                                  1000  \n",
       "5                                  1000  \n",
       "6                                  1000  \n",
       "7                                  7500  \n",
       "8                                  3500  \n",
       "9                                  1000  \n",
       "10                                 3500  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(random_searching.cv_results_)[\n",
    "    [\n",
    "        \"rank_test_score\",\n",
    "        \"mean_fit_time\",\n",
    "        \"mean_test_score\",\n",
    "        \"param_classifier__C\",\n",
    "        \"param_pca__max_features\"\n",
    "    ]\n",
    "].set_index(\"rank_test_score\").sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cv score from grid search: 0.862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pca__max_features': 7500, 'classifier__C': 0.1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Best cv score from grid search: %.3f\" % random_searching.best_score_)\n",
    "random_searching.best_params_\n",
    "# random_searching.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Model interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_searching.best_estimator_.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative Words</th>\n",
       "      <th>Negative Word Weights</th>\n",
       "      <th>Positive Words</th>\n",
       "      <th>Positive Word Weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>worst</td>\n",
       "      <td>-1.530608</td>\n",
       "      <td>hooked</td>\n",
       "      <td>0.621352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>waste</td>\n",
       "      <td>-1.417391</td>\n",
       "      <td>surprisingly</td>\n",
       "      <td>0.640163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>awful</td>\n",
       "      <td>-1.184617</td>\n",
       "      <td>perfectly</td>\n",
       "      <td>0.653714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>poorly</td>\n",
       "      <td>-1.153789</td>\n",
       "      <td>gem</td>\n",
       "      <td>0.666010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disappointment</td>\n",
       "      <td>-1.148894</td>\n",
       "      <td>today</td>\n",
       "      <td>0.669176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>boring</td>\n",
       "      <td>-0.970015</td>\n",
       "      <td>appreciated</td>\n",
       "      <td>0.674908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>disappointing</td>\n",
       "      <td>-0.934189</td>\n",
       "      <td>noir</td>\n",
       "      <td>0.694941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dull</td>\n",
       "      <td>-0.879641</td>\n",
       "      <td>loved</td>\n",
       "      <td>0.695086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lacks</td>\n",
       "      <td>-0.849454</td>\n",
       "      <td>funniest</td>\n",
       "      <td>0.698207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mess</td>\n",
       "      <td>-0.820419</td>\n",
       "      <td>wonderful</td>\n",
       "      <td>0.703071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>laughable</td>\n",
       "      <td>-0.806223</td>\n",
       "      <td>incredible</td>\n",
       "      <td>0.703379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>terrible</td>\n",
       "      <td>-0.783181</td>\n",
       "      <td>great</td>\n",
       "      <td>0.704147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>horrible</td>\n",
       "      <td>-0.772185</td>\n",
       "      <td>amazing</td>\n",
       "      <td>0.705991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>fails</td>\n",
       "      <td>-0.765704</td>\n",
       "      <td>rare</td>\n",
       "      <td>0.737503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>avoid</td>\n",
       "      <td>-0.761276</td>\n",
       "      <td>enjoyable</td>\n",
       "      <td>0.741764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>badly</td>\n",
       "      <td>-0.757015</td>\n",
       "      <td>superb</td>\n",
       "      <td>0.772068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>worse</td>\n",
       "      <td>-0.756324</td>\n",
       "      <td>wonderfully</td>\n",
       "      <td>0.772680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>unfunny</td>\n",
       "      <td>-0.742842</td>\n",
       "      <td>refreshing</td>\n",
       "      <td>0.791014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>poor</td>\n",
       "      <td>-0.742643</td>\n",
       "      <td>perfect</td>\n",
       "      <td>0.914770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>forgettable</td>\n",
       "      <td>-0.737213</td>\n",
       "      <td>excellent</td>\n",
       "      <td>1.050803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Negative Words  Negative Word Weights Positive Words  \\\n",
       "0            worst              -1.530608         hooked   \n",
       "1            waste              -1.417391   surprisingly   \n",
       "2            awful              -1.184617      perfectly   \n",
       "3           poorly              -1.153789            gem   \n",
       "4   disappointment              -1.148894          today   \n",
       "5           boring              -0.970015    appreciated   \n",
       "6    disappointing              -0.934189           noir   \n",
       "7             dull              -0.879641          loved   \n",
       "8            lacks              -0.849454       funniest   \n",
       "9             mess              -0.820419      wonderful   \n",
       "10       laughable              -0.806223     incredible   \n",
       "11        terrible              -0.783181          great   \n",
       "12        horrible              -0.772185        amazing   \n",
       "13           fails              -0.765704           rare   \n",
       "14           avoid              -0.761276      enjoyable   \n",
       "15           badly              -0.757015         superb   \n",
       "16           worse              -0.756324    wonderfully   \n",
       "17         unfunny              -0.742842     refreshing   \n",
       "18            poor              -0.742643        perfect   \n",
       "19     forgettable              -0.737213      excellent   \n",
       "\n",
       "    Positive Word Weights  \n",
       "0                0.621352  \n",
       "1                0.640163  \n",
       "2                0.653714  \n",
       "3                0.666010  \n",
       "4                0.669176  \n",
       "5                0.674908  \n",
       "6                0.694941  \n",
       "7                0.695086  \n",
       "8                0.698207  \n",
       "9                0.703071  \n",
       "10               0.703379  \n",
       "11               0.704147  \n",
       "12               0.705991  \n",
       "13               0.737503  \n",
       "14               0.741764  \n",
       "15               0.772068  \n",
       "16               0.772680  \n",
       "17               0.791014  \n",
       "18               0.914770  \n",
       "19               1.050803  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = random_searching.best_estimator_['classifier'].coef_.flatten()\n",
    "#classifier = logreg\n",
    "vocab = random_searching.best_estimator_['pca'].get_feature_names()\n",
    "#pca = countvec\n",
    "inds = np.argsort(random_searching.best_estimator_['classifier'].coef_.flatten())\n",
    "\n",
    "negative_words = [vocab[index] for index in inds[:20]]\n",
    "positive_words = [vocab[index] for index in inds[-20:]]\n",
    "negative_words_weights = [weights[index] for index in inds[:20]]\n",
    "positive_words_weights = [weights[index] for index in inds[-20:]]\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Negative Words\": negative_words, \"Negative Word Weights\": negative_words_weights,\n",
    "        \"Positive Words\": positive_words,\"Positive Word Weights\": positive_words_weights,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The classification seems correct, with negative words like \"worst\" as highly negative, while words like \"hooked\", are positive. \n",
    "\n",
    "> This task is best suited for models that handle binary data, with learning from the examples and not the features. So, it would be harder for RBFs or NBs to get most informative features, while DT might actually do better, due to it's binary nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test score and final evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_5 = random_searching.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87756"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_5.fit(X_train, y_train)\n",
    "pipe_5.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Score of 0.877 is better than the previous 0.862.\n",
    "\n",
    "> The test scores are very similiar, which means we don't have much overfitting or underfitting. I trust these test scores because we also didn't violate any golden rules + we have a large data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.DataFrame({\"reviews\":X_train,\n",
    "                       \"prediction\": pipe_5.predict(X_train),\n",
    "                       \"probabilities\": pipe_5.predict_proba(X_train).tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43417</th>\n",
       "      <td>By now you've probably heard a bit about the n...</td>\n",
       "      <td>pos</td>\n",
       "      <td>[2.4873436643702007e-12, 0.9999999999975127]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40085</th>\n",
       "      <td>By 1987 Hong Kong had given the world such fil...</td>\n",
       "      <td>pos</td>\n",
       "      <td>[6.94970037073972e-09, 0.9999999930502996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40546</th>\n",
       "      <td>Mukhsin is a beautiful movie about a first lov...</td>\n",
       "      <td>pos</td>\n",
       "      <td>[2.739420634778611e-08, 0.9999999726057937]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38084</th>\n",
       "      <td>Romance is in the air and love is in bloom in ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>[3.8195575480237665e-08, 0.9999999618044245]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46752</th>\n",
       "      <td>\"Twelve Monkeys\" is odd and disturbing, yet be...</td>\n",
       "      <td>pos</td>\n",
       "      <td>[7.257617962164176e-08, 0.9999999274238204]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 reviews prediction  \\\n",
       "43417  By now you've probably heard a bit about the n...        pos   \n",
       "40085  By 1987 Hong Kong had given the world such fil...        pos   \n",
       "40546  Mukhsin is a beautiful movie about a first lov...        pos   \n",
       "38084  Romance is in the air and love is in bloom in ...        pos   \n",
       "46752  \"Twelve Monkeys\" is odd and disturbing, yet be...        pos   \n",
       "\n",
       "                                      probabilities  \n",
       "43417  [2.4873436643702007e-12, 0.9999999999975127]  \n",
       "40085    [6.94970037073972e-09, 0.9999999930502996]  \n",
       "40546   [2.739420634778611e-08, 0.9999999726057937]  \n",
       "38084  [3.8195575480237665e-08, 0.9999999618044245]  \n",
       "46752   [7.257617962164176e-08, 0.9999999274238204]  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_review_predict = reviews.query(\"prediction == 'pos'\")\n",
    "positive_review_predict.sort_values('probabilities').iloc[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29702</th>\n",
       "      <td>Zombi 3 starts as a group of heavily armed men...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[0.9999999998966679, 1.0333211105564274e-10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30388</th>\n",
       "      <td>Sexo Cannibal, or Devil Hunter as it's more co...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[0.9999999982309533, 1.76904665983887e-09]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35171</th>\n",
       "      <td>Munchies starts in deepest darkest Peru (looks...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[0.9999999977927764, 2.2072236201023745e-09]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26780</th>\n",
       "      <td>This is crap....utter crap. I cannot believe a...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[0.9999999945945717, 5.405428302356882e-09]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33321</th>\n",
       "      <td>Scarecrow is set in the small American town of...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[0.9999999877892887, 1.2210711313025467e-08]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 reviews prediction  \\\n",
       "29702  Zombi 3 starts as a group of heavily armed men...        neg   \n",
       "30388  Sexo Cannibal, or Devil Hunter as it's more co...        neg   \n",
       "35171  Munchies starts in deepest darkest Peru (looks...        neg   \n",
       "26780  This is crap....utter crap. I cannot believe a...        neg   \n",
       "33321  Scarecrow is set in the small American town of...        neg   \n",
       "\n",
       "                                      probabilities  \n",
       "29702  [0.9999999998966679, 1.0333211105564274e-10]  \n",
       "30388    [0.9999999982309533, 1.76904665983887e-09]  \n",
       "35171  [0.9999999977927764, 2.2072236201023745e-09]  \n",
       "26780   [0.9999999945945717, 5.405428302356882e-09]  \n",
       "33321  [0.9999999877892887, 1.2210711313025467e-08]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_review_predict = reviews.query(\"prediction == 'neg'\")\n",
    "negative_review_predict.sort_values('probabilities', ascending = False).iloc[0:5]"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python [conda env:571]",
   "language": "python",
   "name": "conda-env-571-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
